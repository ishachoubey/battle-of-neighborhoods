{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json\n\n!conda install -c conda-forge geopy --yes\nfrom geopy.geocoders import Nominatim\n\nimport requests\nfrom pandas.io.json import json_normalize\n\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import KMeans\n\nprint('Libraries Imported!')","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 4.8.2\n  latest version: 4.8.3\n\nPlease update conda by running\n\n    $ conda update -n base conda\n\n\n\n## Package Plan ##\n\n  environment location: /srv/conda/envs/notebook\n\n  added / updated specs:\n    - geopy\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    geographiclib-1.50         |             py_0          34 KB  conda-forge\n    geopy-1.22.0               |     pyh9f0ad1d_0          63 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:          97 KB\n\nThe following NEW packages will be INSTALLED:\n\n  geographiclib      conda-forge/noarch::geographiclib-1.50-py_0\n  geopy              conda-forge/noarch::geopy-1.22.0-pyh9f0ad1d_0\n\n\n\nDownloading and Extracting Packages\ngeographiclib-1.50   | 34 KB     | ##################################### | 100% \ngeopy-1.22.0         | 63 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nLibraries Imported!\n","output_type":"stream"}]},{"cell_type":"code","source":"Importing Dataframes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NY_grouped = pd.read_excel('NY_grouped.xlsx')\nToronto_grouped = pd.read_excel('Toronto_grouped.xlsx')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merging New York and Toronto data in order to perform clustering","metadata":{}},{"cell_type":"code","source":"L1 = list(NY_grouped.columns)\nL2 = list(Toronto_grouped.columns)\nL=list(set(L1).intersection(L2))\n\ndf=pd.merge(NY_grouped,Toronto_grouped, on=L,how='outer')\ncols = [col for col in df.columns if col not in L]\ndf=df.drop(cols,axis=1)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nneigh_name = df['Neighborhood']\ndf_grouped_clustering = df.drop('Neighborhood', 1)\n\nscaler = MinMaxScaler(feature_range=[0, 1])\ndata_rescaled = scaler.fit_transform(df_grouped_clustering)\n\npca = PCA().fit(data_rescaled)\nplt.figure(figsize=(8,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.yticks(np.arange(0, 1.0, step=0.05))\n# plt.xticks(np.arange(0, 450, step=30))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=175)\ndataset = pca.fit_transform(data_rescaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(1, 30)\nfor kclusters in K:\n    km = KMeans(n_clusters=kclusters, init='k-means++', n_init=10, max_iter=300, tol=0.0001,  random_state=10).fit(dataset)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as numpy\nimport sklearn\nimport matplotlib.pyplot as plt\n \nobs = dataset.copy() \n# obs = numpy.concatenate( (numpy.random.randn(100, 2) , 20 + numpy.random.randn(300, 2) , -15+numpy.random.randn(200, 2)))\nsilhouette_score_values=list()\n \nNumberOfClusters=range(3,30)\n \nfor i in NumberOfClusters:\n    \n    classifier=KMeans(i,init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=10)\n    classifier.fit(obs)\n    labels= classifier.predict(obs)\n#     print(\"Number Of Clusters:\")\n#     print(i)\n#     print(\"Silhouette score value\")\n#     print(sklearn.metrics.silhouette_score(obs,labels ,metric='euclidean', sample_size=None, random_state=None))\n    silhouette_score_values.append(sklearn.metrics.silhouette_score(obs,labels ,metric='euclidean', random_state=0))\n \nplt.plot(NumberOfClusters, silhouette_score_values)\nplt.title(\"Silhouette score values vs Numbers of Clusters \")\nplt.show()\n \nOptimal_NumberOf_Components=NumberOfClusters[silhouette_score_values.index(max(silhouette_score_values))]\nprint(\"Optimal number of components is:\")\nprint(Optimal_NumberOf_Components)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing K-means","metadata":{}},{"cell_type":"code","source":"# set number of clusters\nkclusters = 5\n\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=2, copy_x=True).fit(df_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grouped_with_cluster = df.copy()\ndf_grouped_with_cluster.insert(0, 'Cluster_Labels', kmeans.labels_)\ndf_grouped_with_cluster.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_cluster_and_neighborhood = df_grouped_with_cluster[['Cluster_Labels', 'Neighborhood']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Appending New York and Toronto data","metadata":{}},{"cell_type":"code","source":"NY_data = pd.read_excel('NY_data.xlsx')\nToronto_data = pd.read_excel('Toronto_data.xlsx')\nprint('NY_data shape: ', NY_data.shape)\nprint('Toronto_data shape: ', Toronto_data.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NY_data_merged = NY_data[['Neighborhood', 'Latitude', 'Longitude']]\nNY_data_merged = NY_data_merged.merge(df_with_cluster_and_neighborhood, on = 'Neighborhood', how = 'left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NY_data_merged.dropna(inplace = True)\nNY_data_merged.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Toronto_data_merged = Toronto_data[['Neighborhood', 'Latitude', 'Longitude']]\nToronto_data_merged = Toronto_data_merged.merge(df_with_cluster_and_neighborhood, on = 'Neighborhood', how = 'left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Toronto_data_merged.dropna(inplace = True)\nToronto_data_merged.shape","metadata":{},"execution_count":null,"outputs":[]}]}